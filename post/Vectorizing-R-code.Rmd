---
title: "Vectorizing R code and why it works"
date: '2017-06-06'
output: 
  html_document:
    theme: default
    highlight: null
comments: true
prev_next: true
categories: [R]
tags: [vectorization, refactoring, linear algebra]
---

### The beginning ...

Somewhere last January I began my R journey, which means that I have been programming with it for almost five months now. Prior to this period, my only experiences with R have been very frustrating, to say the least. These might have occurred while I was looking for the implementation of an obscure econometrical method or the one or two times that I tried to learn R and failed miserably. Me not being able to learn R at once is not that surprising, considering that R is a quirky language with a though learning curve. However, these things were not the only cause of my failures, what felt as the main cause was the difference between R and my previous programming language: [MATLABÂ®](https://nl.mathworks.com/?s_tid=gn_logo).

Matlab is a matrix based language optimized for operations involving matrices and vectors. This means that it is possible to perform computations directly on whole vectors and matrices instead on scalars (numbers) alone. Hence, in MATLAB it is encouraged to, for example, avoid endless nested loops and instead use matrix based methods. 

The programming paradigm of MATLAB is therefore completely different from the R philosophy, that customarily dictates the use of the [apply](https://stackoverflow.com/a/7141669/8108484) family that loops over individual data points. This has resulted in the somewhat unfortunate reputation that R is inherently [slow](http://fluff.info/blog/arch/00000172.htm). While there is some truth to this statement, because compiled languages are of course faster, the performance of R has improved a lot over the years. For example, by introducing built in functions (e.g. `rowSums`) that use matrix based methods and that have been written in low-level languages like C and Fortran. 

### Vectorization

The process of revising loop-based, scalar-oriented code to use matrix and vector operations is called *vectorization*. Vectorizing your code is worthwhile for several reasons:

- Appearance; vectorized mathematical code appears more like the mathematical expressions found in textbooks, making the code easier to understand.
- Less Error Prone; without loops, vectorized code is often shorter. Fewer lines of code mean fewer opportunities to introduce programming errors.
- Performance; vectorized code often runs much faster than the corresponding code containing loops.
- Scalability; matrix based methods are generic and therefore usually nest many different models.

#### Mathematics behind vectorization

Under the hood, vectorization works by utilizing advanced techniques based on mathematical matrix theory, and therefore, to truly grasp the ideas behind vectorization, it is necessary to get our hands dirty --- we will have to take a look at the mathematics that makes up most of our beloved models.

The key subarea of mathematics involved with computers and programming is linear algebra, the language of vectors and matrices. Below, I will give a short introduction to give you some intuition, however, if you are interested to learn about it in some more detail, then I advise this [book](http://www.cs.cas.cz/duintjertebbens/pubs/Lang1.pdf) for a thorough introduction to linear algebra by Serge Lang, former professor emeritus of mathematics at Yale University. 

<div style="border-style:solid; border-width: 2px;border-radius:5px;padding: 0px 10px 0px 10px;"> 
***Linear algebra introduction***
<br>
In everyday life, we continuously use numbers and perform operations, such as addition $+$, subtraction $-$, and multiplication $\times$ that all take two and return one number. 
<br>

In linear algebra, this is seen as the $1$-dimensional special case of a more general $n$-dimensional space. The building blocks of this $n$-dimensional space are so-called $n$-tuples,$$(x_1, x_2, ..., x_n) ,$$otherwise known as vectors. The numbers $x_1, x_2, ..., x_n$ are called coordinates, and in $1$-dimensional space they represent a point on a line, in $2$-dimensional space they represent a point on a plane, in $3$-dimensional space they represent a point on a $3$d surface, $...$, and finally, in $n$-dimensional space they represent a point on a $n$d surface. 
<br>

<!-- As for numbers, there exist operations on vectors. Vector operations are a more general case of the ordinary operations. There is addition, which behaves very similar to scalar addition, as it takes two $n \times 1$ vectors and returns a single $n \times 1$ vector by performing element-wise addition. Vector multiplication, in the element-wise sense, is called the dot product, which takes two $n \times 1$ vectors and returns a single $n \times 1$ vector by performing element-wise multiplication. -->
<!-- <br> -->

In turn, $n$-dimensional vectors are a special case of $m,n$-dimensional rectangular arrays, called matrices. Here, $m$ and $n$ refer to the number of rows and columns of the matrix, respectively. A matrix with two rows and three columns, a $2 \times 3$ matrix, is represented as follows $$ \begin{bmatrix} a_{1,1} & a_{1,2} & a_{1,3} \\  a_{2,1} & a_{2,2} & a_{2,3} \end{bmatrix}.$$
Operations can also be performend on matrices. In fact, the operations on scalars (or $1 \times 1$ matrices or numbers) are a special case of the respective matrix operations. Today, we will look at three different matrix operations: addition, and two types of multiplication, the dot-product and the matrix product.

Addition is one of the simplest matrix operations, as it basically is element-wise scalar addition
$$\begin{bmatrix} a_{1,1} & a_{1,2} & a_{1,3} \\  a_{2,1} & a_{2,2} & a_{2,3} \end{bmatrix} + \begin{bmatrix} b_{1,1} & b_{1,2} & b_{1,3} \\  b_{2,1} & b_{2,2} & b_{2,3} \end{bmatrix} = \begin{bmatrix} a_{1,1} + b_{1,1}& a_{1,2} + b_{1,2} & a_{1,3} + b_{1,3} \\  a_{2,1} + b_{2,1} & a_{2,2} + b_{2,2} & a_{2,3} + b_{2,3} \end{bmatrix}.$$

Matrix multiplication is a bit more difficult, not least because there are multiple ways to multiply matrices. The dot-product, just like matrix addition, is element-wise scalar multiplication
$$\begin{bmatrix} a_{1,1} & a_{1,2} & a_{1,3} \\  a_{2,1} & a_{2,2} & a_{2,3} \end{bmatrix} \bullet \begin{bmatrix} b_{1,1} & b_{1,2} & b_{1,3} \\  b_{2,1} & b_{2,2} & b_{2,3} \end{bmatrix} = \begin{bmatrix} a_{1,1} b_{1,1}& a_{1,2} b_{1,2} & a_{1,3} b_{1,3} \\  a_{2,1} b_{2,1} & a_{2,2} b_{2,2} & a_{2,3} b_{2,3} \end{bmatrix}.$$
The matrix product is something that we are unfamiliar with in the 'scalar world'. The previous operations both needed two matrices with equal dimensions and returned one matrix with the same dimension. The matrix product is quite different:

- Importance of order; matrix $A$ multiplied with matrix $B$ is not necessarily equal to matrix $B$ multiplied with matrix $A$.^[They are only equal when both matrices are square (number of rows and columns are equal), or in the trivial case (when both matrices are scalars)]
- Compatibility of dimensions; matrix $A$ and matrix $B$ can only be multiplied when the number of columns of the first and the number of rows of the second matrix are equal. 
- (Even higher) importance of order; the result of a matrix multiplication is a matrix with the number of rows of the first and the number of columns of the second matrix. 

A $2 \times 3$ matrix multiplied with a $3 \times 2$ matrix has output

$$\begin{bmatrix} a_{1,1} & a_{1,2} & a_{1,3} \\  a_{2,1} & a_{2,2} & a_{2,3} \end{bmatrix} \times \begin{bmatrix} b_{1,1} & b_{1,2} \\  b_{2,1} & b_{2,2} \\ b_{3,1} & b_{3,2} \end{bmatrix} = \\ 
\begin{bmatrix} a_{1,1}b_{1,1} + a_{1,2}b_{1,2} + a_{1,3}b_{1,3} & a_{1,1}b_{1,2} + a_{1,2}b_{2,2} + a_{1,3}b_{3,2} \\ a_{2,1}b_{1,1} + a_{2,2}b_{1,2} + a_{2,3}b_{1,3} & a_{2,1}b_{1,2} + a_{2,2}b_{2,2} + a_{2,3}b_{3,2}  \end{bmatrix},$$
and a $3 \times 2$ matrix multiplied with a $2 \times 3$ matrix has output
$$\begin{bmatrix} b_{1,1} & b_{1,2} \\  b_{2,1} & b_{2,2} \\ b_{3,1} & b_{3,2} \end{bmatrix} \times \begin{bmatrix} a_{1,1} & a_{1,2} & a_{1,3} \\  a_{2,1} & a_{2,2} & a_{2,3} \end{bmatrix} = \\ 
\begin{bmatrix} b_{1,1}a_{1,1} + b_{1,2}a_{2,1} & b_{1,1}a_{1,2} + b_{1,2}a_{1,3}\\ 
b_{2,1}a_{1,1} + b_{2,2}a_{2,1} & b_{2,1}a_{1,2} + b_{2,2}a_{1,3}\\
b_{3,1}a_{1,1} + b_{3,2}a_{2,1} & b_{3,1}a_{1,2} + b_{3,2}a_{1,3}\\ \end{bmatrix}.$$

So, we conclude that the beautiful thing about matrices is that they nest vectors and scalars as special cases, which means that every model that is based on matrices will also apply to vectors and scalars, which is not necessarily true the other way around. 
</div>

#### Vectorization in R 
But why are we going into so much detail on advanced mathematics and vectorization when talking about R? Well, if you have ever used nested for loops before, then you know how much computation time and power they require. This can be solved by replacing the relatively slow loops by vectorized methods, which use the highly optimized built in matrix operations. Below I will give some examples.

**EXAMPLE 1.** We want to compute the mean between two data series over time.

```{r eval=FALSE}
# Set the random seed for reproducibility
set.seed(1)

# Initialize the data
n <- 10000
num_series <- 2
data <- data.frame(matrix(runif(n*num_series),ncol=num_series))

# Approach 1: for loop
system.time(for (t in 1:n) {
    data$mean[t] <- mean(as.numeric(data[t,1:num_series]))
})
##    user  system elapsed 
##   1.732   0.202   7.786 

# Approach 2: vectorization
system.time(data$mean2 <- rowMeans(data))
##    user  system elapsed 
##   0.000   0.000   0.001
```
This example shows how powerful refactoring your code to use vectorized methods is, there is a speed improvement of over $5000$ times! What would happen to the performance of both methods when we increase the number of data points?

```{r eval=FALSE}
# Initialize the data
n <- 50000
num_series <- 2
data <- data.frame(matrix(runif(n*num_series),ncol=num_series))

# Approach 1: for loop
system.time(for (t in 1:n) {
    data$mean[t] <- mean(as.numeric(data[t,1:num_series]))
})
##    user  system elapsed 
##  17.719   1.861  19.781 

# Approach 2: vectorization
system.time(data$mean2 <- rowMeans(data))
##    user  system elapsed 
##   0.002   0.001   0.003 
```
Thus, the for loop based approach already takes considerable amount of time to complete, whereas the vectorized approach still finishes in an instant. This means, that if we were using the for loop based approach, that we would really be in trouble if the number of data points were to increase even more. 

**EXAMPLE 2.** We are interested in the total value of our investment portfolio $p$, which consists of $n$ different financial instruments, over a number of years. This can be done by multiplying the price of each financial instrument $i$ at time $t$ with the volume of financial instrument $i$ at time $t$ and taking the total sum.

```{r eval=FALSE}
# Set the random seed for reproducibility
set.seed(1)
# Initialize the prices and volumes
num_years <- 10
num_items <- 2
prices <- data.frame(matrix(runif(num_years*num_items)+5, nrow=num_years))
volume <- data.frame(matrix(runif(num_years*num_items), nrow=num_years))

# Approach 1: for loop
value_portfolio <- 0
for (i in 1:num_items){
  value_item <- 0
  for (t in 1:num_years){
    value_item <- value_item + prices[t,i]*volume[t,i]
  }
  value_portfolio <- value_portfolio + value_item
}
## [1] 52.29415

# Approach 2: vectorization
value_portfolio <- sum(prices*volume)
## [1] 52.29415
```
If, some time later, we suddenly wish to compute the portfolio value for each individual year, instead of the total portvolio value, then we could change the code as follows

```{r eval=FALSE}
# Approach 1: for loop
value_portfolio <- data.frame(value=matrix(0, row=num_years,ncol=1))
for (t in 1:num_years){
  value_items <- 0
  for (i in 1:num_item){
    value_items <- value_items + prices[t,i]*volume[t,i]
  }
  value_portfolio[t,1] <- value_items
}
## [1] 7.431395 4.243342 ... 8.789179 4.098889

# Approach 2: vectorization
value_portfolio <- rowSums(prices*volume)
## [1] 7.431395 4.243342 ... 8.789179 4.098889 
```

It should now be clear that vectorization not only improves performance, but that it also improves readability, makes code less error prone (can you spot them?), and more flexible.

### Other ways to improve your code

Of course, vectorization is not the only way to improve your code in R. There is a host of other things that can be done:

- [Coding conventions](https://google.github.io/styleguide/Rguide.xml); One day, someone, heck it might even be you, will be looking at your code, that has been written months ago, and won't be able to understand a thing. All because of a total lack of consistency and the brilliant idea to name every other variable something along the lines of 'VAR-squared_Final.v3'. To improve the sharability and longevity of code, it is hence of utmost importance to adhere to, at least, some kind of coding conventions. 
<center>
> "Good code is like a good joke, it needs no explanation"
</center>
- [Parallelization](https://en.wikipedia.org/wiki/Parallel_computing); Parallel computing is a type of computation in which many calculations are carried out simultaneously. A requirement for parallel computing is that the calculations are independent. Those kind of calculations are usually referred to as 'embarrassingly parallel', because they unnecessarily take lots of time when processed sequentially. 
- [Rcpp](http://www.rcpp.org/); the most probable cause of slow code in R are huge numbers of nested for loops. This problem can be mitigated by implementing the loops directly in low-level programming languages like C or C++.
- $...$

I will cover these topics in more detail in future blog posts.
