---
title: "Vectorizing R code and why it works"
date: '2017-06-06'
output: html_document
comments: true
prev_next: true
categories: [R]
tags: [vectorization, refactoring, linear algebra]
---

### The beginning ...

Somewhere last January I began my R adventure, which means that I have been programming with it for almost five months now. Prior to this period, my only experiences with R have been very frustrating, to say the least. These might have occurred while I was looking for the implementation of an obscure econometrical method or the one or two times that I tried to learn R and failed miserably. Me not being able to learn R at once is not that surprising, considering that R is a quirky language with a though learning curve. However, these things were not the only cause of my failure, what felt as the main cause was the difference between R and my previous programming language: [MATLABÂ®](https://nl.mathworks.com/?s_tid=gn_logo).

Matlab is a matrix based language optimized for operations involving matrices and vectors. This means that it is possible to perform computations directly on whole vectors and matrices instead on scalars (numbers) alone. Hence, in MATLAB it is encouraged to, for example, avoid endless nested loops and instead use matrix based methods. 

The programming paradigm of MATLAB is therefore completely different from the R philosophy, that customarily dictates the use of the [apply](https://stackoverflow.com/a/7141669/8108484) family that loops over individual data points. This has resulted in the somewhat unfortunate reputation that R is inherently [slow](http://fluff.info/blog/arch/00000172.htm). While there is some truth to this statement, because compiled languages are of course faster, the performance of R has improved a lot over the years. For example, by introducing built in functions (e.g. `colSums`) that use matrix based methods and that have been written in low-level languages like C and Fortran. 

### Vectorization

The process of revising loop-based, scalar-oriented code to use matrix and vector operations is called *vectorization*. Vectorizing your code is worthwhile for several reasons:

- Appearance: Vectorized mathematical code appears more like the mathematical expressions found in textbooks, making the code easier to understand.
- Less Error Prone: Without loops, vectorized code is often shorter. Fewer lines of code mean fewer opportunities to introduce programming errors.
- Performance: Vectorized code often runs much faster than the corresponding code containing loops.
- Scalability: Matrix based methods are generic and therefore usually nest many different models.

#### Mathematics behind vectorization

Under the hood, vectorization works by utilizing advanced techniques based on mathematical matrix theory, and therefore, to truly grasp the ideas behind vectorization, it is necessary to get our hands dirty --- we will have to take a look at the mathematics that makes up most of our beloved models.

The key subarea of mathematics involved with computers and programming is linear algebra, the language of vectors and matrices. Below, I will give a short introduction to linear algebra to give you some intuition, however, if you are interested to learn about it in some more detail, then I advise this [book](http://www.cs.cas.cz/duintjertebbens/pubs/Lang1.pdf) for a thorough introduction to linear algebra by Serge Lang, former professor emeritus of mathematics at Yale University. 

<div style="border-style:solid; border-width: 2px;border-radius:5px;padding: 0px 10px 0px 10px;"> 
***Linear algebra introduction***
<br>
In everyday life, we continuously use numbers and perform operations, such as addition $+$, subtraction $-$, and multiplication $\times$ that all take two and return one number. 
<br>

In linear algebra, this is seen as the $1$-dimensional special case of a more general $n$-dimensional space. The building blocks of this $n$-dimensional space are so-called $n$-tuples,$$(x_1, x_2, ..., x_n) ,$$otherwise known as vectors. The numbers $x_1, x_2, ..., x_n$ are called coordinates, and in $1$-dimensional space they represent a point on a line, in $2$-dimensional space they represent a point on a plane, in $3$-dimensional space they represent a point on a $3$d surface, $...$, and finally, in $n$-dimensional space they represent a point on a $n$d surface. 
<br>

<!-- As for numbers, there exist operations on vectors. Vector operations are a more general case of the ordinary operations. There is addition, which behaves very similar to scalar addition, as it takes two $n \times 1$ vectors and returns a single $n \times 1$ vector by performing element-wise addition. Vector multiplication, in the element-wise sense, is called the dot product, which takes two $n \times 1$ vectors and returns a single $n \times 1$ vector by performing element-wise multiplication. -->
<!-- <br> -->

In turn, $n$-dimensional vectors are a special case of $m,n$-dimensional rectangular arrays, called matrices. Here, $m$ and $n$ refer to the number of rows and columns of the matrix, respectively. A matrix with two rows and three columns, a $2 \times 3$ matrix, is represented as follows $$A = \begin{bmatrix} a_{1,1} & a_{1,2} & a_{1,3} \\  a_{2,1} & a_{2,2} & a_{2,3} \end{bmatrix}.$$
Operations can also be performend on matrices. In fact, the operations on scalars (or $1 \times 1$ matrices or numbers) are a special case of the respective matrix operations. Today, we will look at three different matrix operations: addition, and two types of multiplication, the dot-product and the matrix product.

Addition is one of the simplest matrix operations, as it basically is element-wise scalar addition
$$\begin{bmatrix} a_{1,1} & a_{1,2} & a_{1,3} \\  a_{2,1} & a_{2,2} & a_{2,3} \end{bmatrix} + \begin{bmatrix} b_{1,1} & b_{1,2} & b_{1,3} \\  b_{2,1} & b_{2,2} & b_{2,3} \end{bmatrix} = \begin{bmatrix} a_{1,1} + b_{1,1}& a_{1,2} + b_{1,2} & a_{1,3} + b_{1,3} \\  a_{2,1} + b_{2,1} & a_{2,2} + b_{2,2} & a_{2,3} + b_{2,3} \end{bmatrix}.$$

Matrix multiplication is a bit more difficult, not least because there are multiple ways to multiply matrices. The dot-product, just like matrix addition, is element-wise scalar multiplication
$$\begin{bmatrix} a_{1,1} & a_{1,2} & a_{1,3} \\  a_{2,1} & a_{2,2} & a_{2,3} \end{bmatrix} \bullet \begin{bmatrix} b_{1,1} & b_{1,2} & b_{1,3} \\  b_{2,1} & b_{2,2} & b_{2,3} \end{bmatrix} = \begin{bmatrix} a_{1,1} b_{1,1}& a_{1,2} b_{1,2} & a_{1,3} b_{1,3} \\  a_{2,1} b_{2,1} & a_{2,2} b_{2,2} & a_{2,3} b_{2,3} \end{bmatrix}.$$
The matrix product is something that we are unfamiliar with in the 'scalar world'. 

<!-- Operations on these matrices are once again a more general case of the before mentioned scalar and vector operations. These operations are complemented with a number of additional operations. The one we are interested in is called the matrix product. This operation behaves a little different than, for example, scalar and vector additition or multiplication, as it takes two matrices that are not restricted to be of similar dimensions. However, the dimensions should be compatible, which means that the columns of the left and the rows of the right matrix in the multiplication should be equal. Or in mathematical notation: for matrices $m \times n$ and $p \times q$ to be compatible $n=p$ should hold. The result of the mentioned matrix product would be a matrix with the number of rows of the left and the number of columns of the right matrix, that means $m \times q$. -->
</div>
